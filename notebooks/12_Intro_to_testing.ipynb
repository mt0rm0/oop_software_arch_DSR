{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Introduction to Tests using pytest\n",
    "\n",
    "In this notebook, we'll explore the basics of testing in Python using pytest. Testing is a critical part of developing reliable and maintainable software. By writing tests, we can ensure that our code behaves as expected, prevent bugs from being introduced, and facilitate code refactoring. We'll cover the basic concepts of testing, how to write and run tests with pytest, and how to organize your test code.\n",
    "\n",
    "## Table of Contents\n",
    "1. Why Testing?\n",
    "2. Introduction to pytest\n",
    "3. Writing Simple Tests\n",
    "4. Running Tests\n",
    "5. Organizing Tests\n",
    "6. Step-by-Step Example\n",
    "7. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why Testing? <a name=\"1\"></a>\n",
    "Testing ensures the following:\n",
    "- **Correctness**: Verifies that the code works as intended.\n",
    "- **Robustness**: Detects edge cases and prevents future bugs.\n",
    "- **Maintainability**: Facilitates code refactoring and feature addition.\n",
    "- **Documentation**: Provides examples of how to use the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Introduction to pytest <a name=\"2\"></a>\n",
    "\n",
    "pytest is a popular testing framework for Python. It is simple to use, supports fixtures for managing test state, and has powerful plugins for extending its functionality.\n",
    "Installation\n",
    "\n",
    "To install pytest, you can use pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or conda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!conda install -c conda-forge pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Writing Simple Tests <a name=\"3\"></a>\n",
    "\n",
    "Let's start by writing some simple tests for a hypothetical function in a data science pipeline.\n",
    "\n",
    "**Example Function**\n",
    "\n",
    "Consider the following function that calculates the mean of a list of numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(numbers):\n",
    "    if not numbers:\n",
    "        raise ValueError(\"The list is empty\")\n",
    "    return sum(numbers) / len(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Writing Tests**\n",
    "\n",
    "Create a new file called test_calculate_mean.py and write this code inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "from your_module import calculate_mean  # Replace 'your_module' with the actual module name\n",
    "\n",
    "def test_calculate_mean():\n",
    "    assert calculate_mean([1, 2, 3]) == 2\n",
    "\n",
    "def test_calculate_mean_empty_list():\n",
    "    with pytest.raises(ValueError, match=\"The list is empty\"):\n",
    "        calculate_mean([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Running Tests <a name=\"4\"></a>\n",
    "\n",
    "To run the tests, navigate to the directory containing the test file and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytest will discover and execute the tests, providing a summary of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Organizing Tests <a name=\"5\"></a>\n",
    "\n",
    "As your test suite grows, organizing your tests becomes important. Here are some common practices:\n",
    "\n",
    "**Directory Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "project/\n",
    "│\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   └── your_module.py\n",
    "│\n",
    "└── tests/\n",
    "    ├── __init__.py\n",
    "    └── test_calculate_mean.py\n",
    "```\n",
    "\n",
    "**Using Fixtures**\n",
    "\n",
    "Fixtures allow you to set up some state before running your tests. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def test_calculate_mean_with_fixture(sample_data):\n",
    "    assert calculate_mean(sample_data) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fixtures are useful for setting up databases, creating temporary files, or preparing any other state that your tests might require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Step-by-Step Example <a name=\"6\"></a>\n",
    "\n",
    "We'll now build a more comprehensive example to demonstrate how to write tests using pytest, including the use of fixtures.\n",
    "\n",
    "**Step 1: Create the Function**\n",
    "\n",
    "Let's assume we have a function in src/data_processing.py that preprocesses data by normalizing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(data):\n",
    "    if not data:\n",
    "        raise ValueError(\"Data is empty\")\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return [(x - mean) / std for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Step 2: Write Tests**\n",
    "\n",
    "Create a new file called test_normalize.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from src.data_processing import normalize\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def test_normalize(sample_data):\n",
    "    result = normalize(sample_data)\n",
    "    mean = np.mean(result)\n",
    "    std_dev = np.std(result)\n",
    "    assert mean == pytest.approx(0)\n",
    "    assert std_dev == pytest.approx(1)\n",
    "\n",
    "def test_normalize_empty_list():\n",
    "    with pytest.raises(ValueError, match=\"Data is empty\"):\n",
    "        normalize([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Step 3: Running Tests**\n",
    "\n",
    "To run the tests, navigate to the directory containing the test file and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "You should see output indicating that the tests have passed or failed, along with details of any failures.\n",
    "\n",
    "**Step 4: Adding More Tests**\n",
    "\n",
    "Expand the test suite to cover more cases, such as handling non-numeric data or extremely large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normalize_non_numeric():\n",
    "    with pytest.raises(TypeError):\n",
    "        normalize(['a', 'b', 'c'])\n",
    "\n",
    "def test_normalize_large_data():\n",
    "    large_data = list(range(1, 10001))\n",
    "    result = normalize(large_data)\n",
    "    mean = np.mean(result)\n",
    "    std_dev = np.std(result)\n",
    "    assert mean == pytest.approx(0, abs=1e-9)\n",
    "    assert std_dev == pytest.approx(1, abs=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that tests run as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Exercise <a name=\"7\"></a>\n",
    "**Task**\n",
    "\n",
    "You are provided with a simple data science function that standardizes a list of numbers by subtracting the mean and dividing by the standard deviation. Your task is to:\n",
    "\n",
    "- Write tests for this function using pytest.\n",
    "- Handle edge cases such as an empty list.\n",
    "- Organize your tests in a separate directory.\n",
    "- Use fixtures to provide sample data for testing.\n",
    "\n",
    "**Initial Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize(numbers):\n",
    "    if not numbers:\n",
    "        raise ValueError(\"The list is empty\")\n",
    "    mean = np.mean(numbers)\n",
    "    std_dev = np.std(numbers)\n",
    "    return [(x - mean) / std_dev for x in numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "- Write tests to check the correctness of the standardize function.\n",
    "- Test edge cases, such as passing an empty list.\n",
    "- Organize the tests in a separate directory.\n",
    "- Use fixtures to provide sample data for testing.\n",
    "\n",
    "**Directory Structure**\n",
    "```markdown\n",
    "project/\n",
    "│\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   └── data_processing.py  # Contains the 'standardize' function\n",
    "│\n",
    "└── tests/\n",
    "    ├── __init__.py\n",
    "    └── test_data_processing.py  # Contains the tests\n",
    "```\n",
    "\n",
    "The test_data_processing.py file could look like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from src.data_processing import standardize\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def test_standardize(sample_data):\n",
    "    result = standardize(sample_data)\n",
    "    mean = np.mean(result)\n",
    "    std_dev = np.std(result)\n",
    "    assert mean == pytest.approx(0)\n",
    "    assert std_dev == pytest.approx(1)\n",
    "\n",
    "def test_standardize_empty_list():\n",
    "    with pytest.raises(ValueError, match=\"The list is empty\"):\n",
    "        standardize([])\n",
    "\n",
    "def test_standardize_non_numeric():\n",
    "    with pytest.raises(TypeError):\n",
    "        standardize(['a', 'b', 'c'])\n",
    "\n",
    "def test_standardize_large_data():\n",
    "    large_data = list(range(1, 10001))\n",
    "    result = standardize(large_data)\n",
    "    mean = np.mean(result)\n",
    "    std_dev = np.std(result)\n",
    "    assert mean == pytest.approx(0, abs=1e-9)\n",
    "    assert std_dev == pytest.approx(1, abs=1e-9)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
